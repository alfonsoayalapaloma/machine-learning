{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://pandas.pydata.org/static/img/pandas.svg\" width=\"250\">\r\n\r\n\r\n## <center> Classifiers","metadata":{}},{"cell_type":"markdown","source":"# Random Forest\r\n\r\n## How it works\r\n\r\n### Bootstrap Sampling: \r\n\r\nRandom Forest creates multiple decision trees using different subsets of the training data. Each subset is created by randomly sampling the data with replacement (bootstrap sampling).\r\n\r\n### Feature Randomness: \r\n\r\nWhen splitting nodes in each decision tree, Random Forest considers a random subset of features rather than all features. This introduces more diversity among the trees.\r\n\r\n### Building Trees: \r\nEach decision tree is built independently using the bootstrap sample and the random subset of features. The trees are grown to their maximum depth without pruning.\r\n\r\n### Aggregation: \r\n\r\nFor classification tasks, the final prediction is made by aggregating the predictions of all individual trees. This is typically done by majority voting (the class that gets the most votes from the trees is the final prediction).\r\n\r\n### Advantages\r\n\r\n### Improved Accuracy: \r\n\r\nBy combining the predictions of multiple trees, Random Forest often achieves higher accuracy than individual decision trees.\r\n\r\n### Robustness: \r\n\r\nIt reduces overfitting and is more robust to noise in the data.\r\nFeature Importance: Random Forest can provide estimates of feature importance, helping to understand which features are most influential in making predictions.\r\n\r\n## Disadvantages\r\n\r\n### Complexity: \r\n\r\nRandom Forest models can be more complex and harder to interpret compared to a single decision tree.\r\n\r\n### Computational Cost:\r\n\r\nTraining multiple trees can be computationally expensive and require more memory.\r\n","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport seaborn as sns \n\nds_iris = load_iris()\n#print(ds_iris.target_names)\n\n# Load the Iris dataset\niris = sns.load_dataset('iris')\nnumeric_cols=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\ntarget_col=\"species\"\n\ntarget_names = iris[target_col].unique() \nX = iris[ numeric_cols ]\ny = iris[ target_col ]\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the classifier\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nreport = classification_report(y_test, y_pred, target_names=target_names)\nprint(\"Classification Report:\\n\", report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T22:58:28.557579Z","iopub.execute_input":"2024-11-01T22:58:28.558003Z","iopub.status.idle":"2024-11-01T22:58:28.749553Z","shell.execute_reply.started":"2024-11-01T22:58:28.557963Z","shell.execute_reply":"2024-11-01T22:58:28.748556Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 1.00\nClassification Report:\n               precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        10\n  versicolor       1.00      1.00      1.00         9\n   virginica       1.00      1.00      1.00        11\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n","output_type":"stream"}],"execution_count":11}]}