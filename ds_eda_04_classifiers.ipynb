{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfonsoayalapaloma/ml-2024/blob/main/ds_eda_04_classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://pandas.pydata.org/static/img/pandas.svg\" width=\"250\">\n",
        "\n",
        "\n",
        "## <center> Classifiers"
      ],
      "metadata": {
        "id": "xuyuPlZeaAwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest\n",
        "\n",
        "## How it works\n",
        "\n",
        "### Bootstrap Sampling:\n",
        "\n",
        "Random Forest creates multiple decision trees using different subsets of the training data. Each subset is created by randomly sampling the data with replacement (bootstrap sampling).\n",
        "\n",
        "### Feature Randomness:\n",
        "\n",
        "When splitting nodes in each decision tree, Random Forest considers a random subset of features rather than all features. This introduces more diversity among the trees.\n",
        "\n",
        "### Building Trees:\n",
        "Each decision tree is built independently using the bootstrap sample and the random subset of features. The trees are grown to their maximum depth without pruning.\n",
        "\n",
        "### Aggregation:\n",
        "\n",
        "For classification tasks, the final prediction is made by aggregating the predictions of all individual trees. This is typically done by majority voting (the class that gets the most votes from the trees is the final prediction).\n",
        "\n",
        "### Advantages\n",
        "\n",
        "### Improved Accuracy:\n",
        "\n",
        "By combining the predictions of multiple trees, Random Forest often achieves higher accuracy than individual decision trees.\n",
        "\n",
        "### Robustness:\n",
        "\n",
        "It reduces overfitting and is more robust to noise in the data.\n",
        "Feature Importance: Random Forest can provide estimates of feature importance, helping to understand which features are most influential in making predictions.\n",
        "\n",
        "## Disadvantages\n",
        "\n",
        "### Complexity:\n",
        "\n",
        "Random Forest models can be more complex and harder to interpret compared to a single decision tree.\n",
        "\n",
        "### Computational Cost:\n",
        "\n",
        "Training multiple trees can be computationally expensive and require more memory.\n"
      ],
      "metadata": {
        "id": "DWrH7w55aAwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "ds_iris = load_iris()\n",
        "#print(ds_iris.target_names)\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = sns.load_dataset('iris')\n",
        "numeric_cols=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
        "target_col=\"species\"\n",
        "\n",
        "target_names = iris[target_col].unique()\n",
        "X = iris[ numeric_cols ]\n",
        "y = iris[ target_col ]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_test, y_pred, target_names=target_names)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-01T22:58:28.557579Z",
          "iopub.execute_input": "2024-11-01T22:58:28.558003Z",
          "iopub.status.idle": "2024-11-01T22:58:28.749553Z",
          "shell.execute_reply.started": "2024-11-01T22:58:28.557963Z",
          "shell.execute_reply": "2024-11-01T22:58:28.748556Z"
        },
        "id": "t-Pgi0WUaAwX",
        "outputId": "294da862-6e46-410a-9f71-2639600bd6fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 1.00\nClassification Report:\n               precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        10\n  versicolor       1.00      1.00      1.00         9\n   virginica       1.00      1.00      1.00        11\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}